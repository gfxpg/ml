{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing TFProf Files\n",
    "\n",
    "TFProf files, generated for instance by [benchmark scripts](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L196), contain memory and time stats for every operation, recorded over one or several runs.\n",
    "\n",
    "I use this notebook to select the operations I'm interested in, compute mean values over all test runs, and convert them to CSV, which is then exported to a spreadsheet app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.pywrap_tensorflow import ProfilerFromFile\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python import pywrap_tensorflow as pwtf\n",
    "from tensorflow import gfile\n",
    "\n",
    "PROFILE_PATH = './resnet50_v2_bnr_b64_log'\n",
    "TMP_FILE_PATH = '/tmp/tfprof_tmp_output'\n",
    "\n",
    "ProfilerFromFile(PROFILE_PATH.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I extract textual information for each run (`step`), filtering the operations by name with regexes and extracting memory (`bytes`) and time (`micros`, `accelerator_micros`, `cpu_micros`) stats. There are additional [options](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md) available, which may come in handy in more complex scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_lines(step = 0):\n",
    "  options = {\n",
    "    'view': 'scope',\n",
    "    'select': ['bytes', 'micros', 'accelerator_micros', 'cpu_micros'], #, 'float_ops'],\n",
    "    'order_by': 'occurrence',\n",
    "    'max_depth': 50,\n",
    "    'min_bytes': 0,\n",
    "    'account_type_regexes': ['.*'],\n",
    "    'start_name_regexes': ['.*'],\n",
    "    'show_name_regexes': [\n",
    "      'v/tower_0/cg/(strided_slice|spatial_mean|Relu).+',\n",
    "      'v/tower_0/cg/.*(FusedBatchNorm|Conv2D|MaxPool|MatMul|Relu|add)$',\n",
    "      'v/tower_0/xentropy/sparse_softmax_cross_entropy_loss',\n",
    "      'v/tower_0/gradients/.*(StridedSliceGrad|FusedBatchNormGrad|spatial_mean\\d+_grad|Conv2DBackpropFilter|Conv2DBackpropInput|MaxPoolGrad|ReluGrad|BiasAddGrad)$'\n",
    "    ],\n",
    "    'output': 'file:outfile=' + TMP_FILE_PATH\n",
    "  }\n",
    "\n",
    "  options['step'] = step\n",
    "\n",
    "  opts = model_analyzer._build_options(options)\n",
    "  pwtf.Profile(options['view'].encode('utf-8'), opts.SerializeToString())\n",
    "\n",
    "  with gfile.GFile(TMP_FILE_PATH, 'rb') as profile:\n",
    "    profile_str = profile.read()\n",
    "\n",
    "  return profile_str.decode('utf-8').split(\"\\n\")[15:]\n",
    "\n",
    "lines_by_steps = [profile_lines(s) for s in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the output contains one operation, e.g.:\n",
    "```\n",
    "'  v/tower_0/cg/resnet_v20/conv2/batchnorm2/FusedBatchNorm (51.38MB/51.38MB, 95us/95us, 0us/0us, 95us/95us)'\n",
    "'  v/tower_0/gradients/v/tower_0/cg/resnet_v20/conv1/conv2d/Conv2D_grad/Conv2DBackpropFilter (199.68KB/199.68KB, 32us/32us, 0us/0us, 32us/32us)'\n",
    "```\n",
    "\n",
    "Stats are placed in the order I've specified above (`bytes, micros, accelerator_micros, cpu_micros`). The units, however, differ between operations (`MB` vs `KB` vs `B`), complicating further analysis. I convert them to a common representation (`KB` for memory usage and `us` for time elapsed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def convert_units(val):\n",
    "  if val.endswith('KB'):\n",
    "    return float(val[:-2])\n",
    "  elif val.endswith(\"MB\"):\n",
    "    return float(val[:-2]) * 1024\n",
    "  elif val.endswith(\"B\"):\n",
    "    return float(val[:-1]) / 1024\n",
    "  elif val.endswith(\"us\"):\n",
    "    return float(val[:-2])\n",
    "  elif val.endswith(\"ms\"):\n",
    "    return float(val[:-2]) * 1000\n",
    "\n",
    "def parse_line(line):\n",
    "  data = line.strip().split(\" (\")\n",
    "  \n",
    "  name = data[0]\\\n",
    "    .replace('v/tower_0/cg/', '')\\\n",
    "    .replace('v/tower_0/gradients/', '')\\\n",
    "    .replace('v/tower_0/xentropy/', '')\\\n",
    "    .replace('conv2d/', '')\\\n",
    "    .replace('Conv2D_grad/', '')\\\n",
    "    .replace('Relu_grad/', '')\\\n",
    "    .replace('FusedBatchNorm_grad/', '')\n",
    "  \n",
    "  name = re.sub(r'/batchnorm\\d+/', '/', name)\n",
    "  \n",
    "  name_type = name.rsplit('/', 1)\n",
    "  if len(name_type) == 1:\n",
    "    if name.startswith('Relu_'): name_type.append('Relu')\n",
    "    else: name_type.append('')\n",
    "  \n",
    "  return (name_type +\n",
    "    [convert_units(stat.split('/')[-1]) for stat in data[1].replace(')', '').split(', ')])\n",
    "\n",
    "parsed_lines_by_steps = np.array([[parse_line(l) for l in lines if l != ''] for lines in lines_by_steps])\n",
    "print(f'Parsed {parsed_lines_by_steps.shape} profile entries; the shape is (runs, ops per run, stats per op)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I compute the mean and the coefficient of variation over test runs and prepare the CSV output. The columns are `short node name, operation name, memory mean, memory cv, total time mean, total time cv, accelerator time mean, accelerator time cv, cpu time mean, cpu time cv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "mean = np.mean(parsed_lines_by_steps[:, :, 2:].astype('float64'), axis=0)\n",
    "std = np.std(parsed_lines_by_steps[:, :, 2:].astype('float64'), axis=0)\n",
    "cf_of_variation = np.nan_to_num(std / mean) * 100.\n",
    "\n",
    "output_shape = parsed_lines_by_steps.shape[1:2] + (2 + 2 * mean.shape[1],)\n",
    "output = np.empty(output_shape, dtype=parsed_lines_by_steps.dtype)\n",
    "\n",
    "output[:, 0:2] = parsed_lines_by_steps[0, :, 0:2]\n",
    "output[:, 2::2] = mean\n",
    "output[:, 3::2] = cf_of_variation\n",
    "\n",
    "np.savetxt(sys.stdout, output, fmt='%s', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
